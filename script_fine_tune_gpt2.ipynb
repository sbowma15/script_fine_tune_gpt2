{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnpoFKg0T0a5lf59P3aFev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbowma15/script_fine_tune_gpt2/blob/main/script_fine_tune_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "p4odzbdNWbZJ",
        "outputId": "9bcc63f9-47e4-4595-c48a-15154072c364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-12-11 22:11:37.962 INFO    __main__: Creating features from file film_text.txt at ../content/sample_data\n",
            "2021-12-11 22:11:49.304 INFO    __main__: Saving features into cached file ../content/sample_data/gpt2_512_film_text.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-ddc0b7ff854e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0mFILE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"film_text.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScriptData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScriptData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mFILE_PATH\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mscript_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ScriptData' from 'torch' (/usr/local/lib/python3.7/dist-packages/torch/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#! python -m pip install transformers \\\n",
        " #  pandas matplotlib numpy \\\n",
        "  # nltk seaborn sklearn gensim pyldavis \\\n",
        " #  wordcloud textblob spacy textstat\n",
        "!pip install --upgrade torch torchvision\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import pandas as pd\n",
        "import gc\n",
        "import json\n",
        "import torch\n",
        "import itertools\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.ma as ma\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler, random_split\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#!pip install texthero\n",
        "#!pip install -U spacy\n",
        "import texthero as hero\n",
        "from texthero import preprocessing\n",
        "from texthero import stopwords\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk, re, string, collections\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    GPT2Config,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2PreTrainedModel,\n",
        "    GPT2Tokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        ")\n",
        "\n",
        "MODEL_CLASSES = {\"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer)}\n",
        "\n",
        "#FILE_PATH = os.path.join(\"script_buddy\", \"data\", \"film_text.txt\")\n",
        "FILE_PATH = '../content/sample_data/film_text.txt'\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Big thanks to the team at huggingface for providing their example\n",
        "# on fine_tuning a dataset, this borrows heavily from that it.\n",
        "# Please find a link to it below :-)\n",
        "# https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py\n",
        "\n",
        "\n",
        "class ScriptData(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        file_path: str,\n",
        "        block_size=512,\n",
        "        overwrite_cache=False,\n",
        "    ):\n",
        "        assert os.path.isfile(file_path)\n",
        "\n",
        "        block_size = block_size - (\n",
        "            tokenizer.model_max_length - tokenizer.max_len_single_sentence\n",
        "        )\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "\n",
        "        # change if args are added at later point\n",
        "        cached_features_file = os.path.join(\n",
        "            directory, \"gpt2\" + \"_\" + str(block_size) + \"_\" + filename\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "            logger.info(\n",
        "                f\"Loading features from your cached file {cached_features_file}\"\n",
        "            )\n",
        "            with open(cached_features_file, \"rb\") as cache:\n",
        "                self.examples = pickle.load(cache)\n",
        "                logger.debug(\"Loaded examples from cache\")\n",
        "        else:\n",
        "            logger.info(f\"Creating features from file {filename} at {directory}\")\n",
        "\n",
        "            self.examples = []\n",
        "            with open(file_path, encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "                logger.debug(\"Succesfully read text from file\")\n",
        "\n",
        "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "\n",
        "            for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
        "                self.examples.append(\n",
        "                    tokenizer.build_inputs_with_special_tokens(\n",
        "                        tokenized_text[i : i + block_size]\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            logger.info(f\"Saving features into cached file {cached_features_file}\")\n",
        "            with open(cached_features_file, \"wb\") as cache:\n",
        "\n",
        "                pickle.dump(self.examples, cache, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "\n",
        "    model.train()\n",
        "    sc = ScriptData(tokenizer, file_path=FILE_PATH,overwrite_cache=True)\n",
        "\n",
        "output_dir = \"cpierse/gpt2_film_scripts\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
        "model = model.to(device)\n",
        "FILE_PATH = os.path.join(\"storage\",\"data\", \"film_text.txt\")\n",
        "from torch import scriptdata\n",
        "dataset = ScriptData(tokenizer= tokenizer, file_path= FILE_PATH )\n",
        "script_loader = DataLoader(dataset,batch_size=4,shuffle=True)\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 0.00002\n",
        "WARMUP_STEPS = 10000\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
        "script_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "    for idx,script in enumerate(script_loader):\n",
        "        outputs = model(script.to(device), labels=script.to(device))\n",
        "\n",
        "        loss, logits = outputs[:2]\n",
        "        loss.backward()\n",
        "        sum_loss = sum_loss + loss.detach().data\n",
        "\n",
        "        script_count = script_count + 1\n",
        "        if script_count == BATCH_SIZE:\n",
        "            script_count = 0\n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        if batch_count == 200:\n",
        "            model.eval()\n",
        "            print(f\"sum loss {sum_loss}\")\n",
        "            sample_outputs = model.generate(\n",
        "                                    bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,\n",
        "                                    top_k=50,\n",
        "                                    max_length = 1000,\n",
        "                                    top_p=0.95,\n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "\n",
        "            print(\"Output:\\n\" + 100 * '-')\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "\n",
        "            batch_count = 0\n",
        "            sum_loss = 0.0\n",
        "            model.train()\n",
        "\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "\n",
        "torch.save(model.state_dict(), output_model_file)\n",
        "model.config.to_json_file(output_config_file)\n",
        "tokenizer.save_vocabulary(output_dir)\n",
        "('./storage/models/vocab.json', './storage/models/merges.txt')\n",
        "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "input_ids = tokenizer.encode('         He kisses her softly and takes out his gun.         ', return_tensors='pt')\n",
        "model.eval()"
      ]
    }
  ]
}